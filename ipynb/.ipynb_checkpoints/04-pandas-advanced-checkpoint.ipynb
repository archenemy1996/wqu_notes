{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27995148",
   "metadata": {},
   "source": [
    "<font size=\"+3\"><strong>Pandas: Advanced</strong></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9339f714",
   "metadata": {},
   "source": [
    "# Calculate Summary Statistics for a DataFrame or Series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10310d71",
   "metadata": {},
   "source": [
    "Many datasets are large, and it can be helpful to get a summary of information in them. Let's load a dataset and examine the first few rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062a74c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "mexico_city1 = pd.read_csv(\"./data/mexico-city-real-estate-1.csv\")\n",
    "mexico_city1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee879034",
   "metadata": {},
   "source": [
    "Let's get a summary description of this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e319e85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mexico_city1.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36be471",
   "metadata": {},
   "source": [
    "Like most large datasets, this one has many values which are missing. The describe function will ignore missing values in each column. You can also remove rows and columns with missing values, and then get a summary of the data that's still there. We need to remove columns first, before removing the rows; the sequence of operations here is important. The code looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601f5b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "mexico_city1 = mexico_city1.drop(\n",
    "    [\"floor\", \"price_usd_per_m2\", \"expenses\", \"rooms\"], axis=1\n",
    ")\n",
    "mexico_city1 = mexico_city1.dropna(axis=0)\n",
    "mexico_city1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ac6b13",
   "metadata": {},
   "source": [
    "Let's take a look at our new, cleaner dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d1b7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mexico_city1.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0884a724",
   "metadata": {},
   "source": [
    "<font size=\"+1\">Practice</font> \n",
    "\n",
    "Reload the `mexico-city-real-estate-1.csv` dataset. Reverse the sequence of operations by first dropping all rows where there is a missing value, and then dropping the columns, `floor`, `price_usd_per_m2`,`expenses` and `rooms`. What is the size of the resulting DataFrame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b185fbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mexico_city1 = pd.read_csv(\"./data/mexico-city-real-estate-1.csv\")\n",
    "mexico_city1 = ...\n",
    "mexico_city1 = mexico_city1.drop(\n",
    "    [\"floor\", \"price_usd_per_m2\", \"expenses\", \"rooms\"], axis=1\n",
    ")  # REMOVERHS\n",
    "print(mexico_city1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c677e0ba",
   "metadata": {},
   "source": [
    "# Select a Series from a DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5720cf",
   "metadata": {},
   "source": [
    "Since the datasets we work with are so large, you might want to focus on a single column of a DataFrame. Let's load up the `mexico-city-real-estate-2` dataset, and examine the first few rows to find the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250bc188",
   "metadata": {},
   "outputs": [],
   "source": [
    "mexico_city2 = pd.read_csv(\"./data/mexico-city-real-estate-2.csv\")\n",
    "mexico_city2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae25eba",
   "metadata": {},
   "source": [
    "Maybe we're interested in the `surface_covered_in_m2` column. The code to extract just that one column looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2a61e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "surface_covered_in_m2 = mexico_city2[\"surface_covered_in_m2\"]\n",
    "surface_covered_in_m2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2133c768",
   "metadata": {},
   "source": [
    "<font size=\"+1\">Practice</font> \n",
    "\n",
    "Select the `price` series from the `mexico-city-real-estate-2` dataset, and load it into the `mexico_city2` DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aeb2b60",
   "metadata": {},
   "source": [
    "price = ...\n",
    "print(price)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc3a0c7",
   "metadata": {},
   "source": [
    "# Subset a DataFrame by Selecting One or More Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f37274",
   "metadata": {},
   "source": [
    "You may find it more efficient to work with a smaller portion of a dataset that's relevant to you. One way to do this is to select some columns from a DataFrame and make a new DataFrame with them. Let's load a dataset to do this and examine the first few rows to find the column headings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cdb52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mexico_city4 = pd.read_csv(\"./data/mexico-city-real-estate-4.csv\")\n",
    "mexico_city4.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6521d7d1",
   "metadata": {},
   "source": [
    "Let's choose `\"operation\"`, `\"property_type\"`, `\"place_with_parent_names\"`, and `\"price\"`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7b8f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "mexico_city4_subset = mexico_city4[\n",
    "    [\"operation\", \"property_type\", \"place_with_parent_names\", \"price\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a390d7dc",
   "metadata": {},
   "source": [
    "Once we've done that, we can find the resulting number of entries in the DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fa1e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "mexico_city4_subset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c2fc91",
   "metadata": {},
   "source": [
    "<font size=\"+1\">Practice</font> \n",
    "\n",
    "Load the `mexico-city-real-estate-1.csv` dataset and subset it to obtain the `operation`, `lat-lon` and `place_with_property_names` columns only. How many entries are in the resulting DataFrame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60c713a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mexico_city1 = ...\n",
    "mexico_city1_subset = mexico_city1[\n",
    "    [\"operation\", \"lat-lon\", \"place_with_parent_names\"]\n",
    "]  # REMOVERHS\n",
    "print(mexico_city1_subset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cba328",
   "metadata": {},
   "source": [
    "# Subset the Columns of a DataFrame Based on Data Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4359577e",
   "metadata": {},
   "source": [
    "It's helpful to be able to find specific types of entries &mdash; typically numeric ones &mdash; and put all of these in a separate DataFrame. First, let's take a look at the `mexico-city-real-estate-5` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f4269d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mexico_city5 = pd.read_csv(\"./data/mexico-city-real-estate-5.csv\")\n",
    "mexico_city5.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28c1232",
   "metadata": {},
   "source": [
    "The code to subset just the numerical entries looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb643597",
   "metadata": {},
   "outputs": [],
   "source": [
    "mexico_city5_numbers = mexico_city5.select_dtypes(include=\"number\")\n",
    "mexico_city5_numbers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eefd381",
   "metadata": {},
   "source": [
    "<font size=\"+1\">Practice</font> \n",
    "\n",
    "Create a subset of the DataFrame from `mexico-city-real-estate-5` which excludes numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cc53e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mexico_city3 = ...\n",
    "mexico_city3_no_numbers = ...\n",
    "print(mexico_city3_no_numbers.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc50e273",
   "metadata": {},
   "source": [
    "# Working with `value_counts` in a Series\n",
    "\n",
    "In order to use the data in a series for other types of analysis, it might be helpful to know how often each value occurs in the Series. To do that, we use the `value_counts` method to aggregate the data. Let's take a look at the number of properties associated with each department in the `colombia-real-estate-1` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9202dcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"data/colombia-real-estate-1.csv\", usecols=[\"department\"])\n",
    "df1[\"department\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a8aec6",
   "metadata": {},
   "source": [
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Try it yourself! Aggregate the different property types in the `colombia-real-estate-2` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9f60ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84502261",
   "metadata": {},
   "source": [
    "# Series and `Groupby`\n",
    "\n",
    "Large Series often include data points that have some attribute in common, but which are nevertheless not grouped together in the dataset. Happily, pandas has a method that will bring these data points together into groups. \n",
    "\n",
    "Let's take a look at the `colombia-real-estate-1` dataset. The set includes properties scattered across Colombia, so it might be useful to group properties from the same department together; to do this, we'll use the `groupby` method. The code looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c48c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "dept_group = df1.groupby(\"department\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923e3e8a",
   "metadata": {},
   "source": [
    "To make sure we got all the departments in the dataset, let's print the first occurrence of each department."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23d76a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dept_group.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7f34df",
   "metadata": {},
   "source": [
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Try it yourself! Group the properties in `colombia-real-estate-2` by department, and print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79aa574",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = ...\n",
    "dept_group = ...\n",
    "dept_group.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55835d5b",
   "metadata": {},
   "source": [
    "Now that we have all the properties grouped by department, we might want to see the properties in just one of the departments. We can use the `get_group` method to do that. If we just wanted to see the properties in `\"Santander\"`, for example, the code would look like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e97e4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"data/colombia-real-estate-1.csv\")\n",
    "dept_group2 = df1.groupby([\"department\", \"property_type\"])\n",
    "dept_group2.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffc6a74",
   "metadata": {},
   "source": [
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Try it yourself! Group the properties in `colombia-real-estate-2` by department and property type, and print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd7f29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dept_group = ...\n",
    "dept_group.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e432740",
   "metadata": {},
   "source": [
    "Finally, it's possible to use `groupby` to calculate aggregations. For example, if we wanted to find the average property area in each department, we would use the `.mean()` method. This is what the code for that looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e96006a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dept_group = df1.groupby(\"department\")[\"area_m2\"].mean()\n",
    "dept_group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ae9480",
   "metadata": {},
   "source": [
    "*Practice*\n",
    "Try it yourself! Use the `.mean` method in the `colombia-real-estate-2` dataset to find the average price in Colombian pesos (`\"price_cop\"`) for properties in each `\"department\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c440ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "dept_group = ...\n",
    "dept_group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553abfd2",
   "metadata": {},
   "source": [
    "# Subset a DataFrame's Columns Based on the Column Data Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d711cb87",
   "metadata": {},
   "source": [
    "It's helpful to be able to find entries of a certain type, typically numerical entries, and put all of these in a separate DataFrame. Let's load a dataset to see how this works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0dad5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mexico_city5 = pd.read_csv(\"./data/mexico-city-real-estate-5.csv\")\n",
    "mexico_city5.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b0f939",
   "metadata": {},
   "source": [
    "Now let's get only numerical entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f76e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mexico_city5_numbers = mexico_city5.select_dtypes(include=\"number\")\n",
    "mexico_city5_numbers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edc38ee",
   "metadata": {},
   "source": [
    "Let's now find all entries which are not numerical entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f3a1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mexico_city5_no_numbers = mexico_city5.select_dtypes(exclude=\"number\")\n",
    "mexico_city5_no_numbers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93908d85",
   "metadata": {},
   "source": [
    "<font size=\"+1\">Practice</font> \n",
    "\n",
    "Create a subset of the DataFrame from `mexico-city-real-estate-5.csv` which excludes numbers. How many entries does it have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a377b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mexico_city3 = ...\n",
    "mexico_city3_no_numbers = ...\n",
    "print(mexico_city3_no_numbers.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ce5728",
   "metadata": {},
   "source": [
    "# Subset a DataFrame's columns based on column names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5de45d",
   "metadata": {},
   "source": [
    "To subset a DataFrame by column names, either define a list of columns to include or define a list of columns to exclude. Once you've done that, you can retain or drop the columns accordingly. For example, let's suppose we want to modify the `mexico_city3` dataset and only retain the first three columns. Let's define two lists, one with the columns to retain and one with the columns to drop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffe1d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = [\n",
    "    \"lat-lon\",\n",
    "    \"price\",\n",
    "    \"currency\",\n",
    "    \"price_aprox_local_currency\",\n",
    "    \"price_aprox_usd\",\n",
    "    \"surface_total_in_m2\",\n",
    "    \"surface_covered_in_m2\",\n",
    "    \"price_usd_per_m2\",\n",
    "    \"price_per_m2\",\n",
    "    \"floor\",\n",
    "    \"rooms\",\n",
    "    \"expenses\",\n",
    "    \"properati_url\",\n",
    "]\n",
    "\n",
    "keep_cols = [\"operation\", \"property_type\", \"place_with_parent_names\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fef8fa7",
   "metadata": {},
   "source": [
    "Next, we'll explore both approaches to subset `mexico_city3`. To retain columns based on `keep_cols`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbbbfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mexico_city3_subsetted = mexico_city3[keep_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce17669b",
   "metadata": {},
   "source": [
    "To drop columns in `drop_cols`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79432b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "mexico_city3_subsetted = mexico_city3.drop(columns=drop_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fcb3ed",
   "metadata": {},
   "source": [
    "<font size=\"+1\">Practice</font> \n",
    "\n",
    "Create a subset of the DataFrame from `mexico-city-real-estate-3.csv` which excludes the last two columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224594f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f0f7f3e",
   "metadata": {},
   "source": [
    "## Pivot Tables\n",
    "\n",
    "A pivot table allows to aggregate and summarize a DataFrame across multiple variables. For example, let's suppose we wanted to calculate the mean of the `price` column in the `mexico_city3` dataset for the different values in the `property_type` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c10642a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "mexico_city3_pivot = ...\n",
    "mexico_city3_pivot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897a82e8",
   "metadata": {},
   "source": [
    "# Subsetting with Masks\n",
    "\n",
    "Another way to to create subsets from a larger dataset is through **masking**. Masks are ways to filter out the data you're not interested in so that you can focus on the data you are. For example, we might want to look at properties in Colombia that are bigger than 200 square meters. In order to create this subset, we'll need to use a mask. \n",
    "\n",
    "First, we'll reset our `df1` DataFrame so that we can draw on all the data in its original form. Then we'll create a statement and then assign the result to `mask`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2568a59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"data/colombia-real-estate-1.csv\")\n",
    "mask = df1[\"area_m2\"] > 200\n",
    "mask.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a6ecd3",
   "metadata": {},
   "source": [
    "Notice that `mask` is a Series of Boolean values. Where properties are smaller than 200 square meters, our statement evaluates as `False`; where they're bigger than 200, it evaluates to `True`.\n",
    "\n",
    "Once we have our mask, we can use it to select all the rows from `df1` that evaluate as `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc5a581",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[mask].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e211605",
   "metadata": {},
   "source": [
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Try it yourself! Read `colombia-real-estate-2` into a DataFrame named `df2`, and create a mask to select all the properties that are smaller than 100 square meters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9918ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = ...\n",
    "mask = ...\n",
    "df2[mask].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb9a082",
   "metadata": {},
   "source": [
    "We can also create masks with multiple criteria using `&` for \"and\" and `|` for \"or.\" For example, here's how we would find all properties in Atlántico that are bigger than 400 square meters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0aad28",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df1[\"area_m2\"] > 400) & (df1[\"department\"] == \"Atlántico\")\n",
    "df1[mask].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abef5604",
   "metadata": {},
   "source": [
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Try it yourself! Create a mask for `df2` to select all the properties in Tolima that are smaller than 150 square meters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4da4881",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = ...\n",
    "df2[mask].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b94fe1",
   "metadata": {},
   "source": [
    "## Reshape a DataFrame based on column values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcfda0e",
   "metadata": {},
   "source": [
    "## What's a pivot table?\n",
    "\n",
    "A pivot table allows you to quickly aggregate and summarize a DataFrame using an aggregation function. For example, to build a pivot table that summarizes the mean of the `price_cop` column for each of the unique categories in the `property_type` column in `df2`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60d42ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "pivot1 = pd.pivot_table(df2, values=\"price_cop\", index=\"property_type\", aggfunc=np.mean)\n",
    "pivot1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd0fc4e",
   "metadata": {},
   "source": [
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Try it yourself! build a pivot table that summarizes the mean of the `price_cop` column for each of the unique departments in the `department` column in `df2`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3adbce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVE {\n",
    "pivot2 = pd.pivot_table(df2, values=\"price_cop\", index=\"department\", aggfunc=np.mean)\n",
    "# REMOVE }\n",
    "pivot2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d82b7a",
   "metadata": {},
   "source": [
    "## Combine multiple categories in a Series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9aecb76",
   "metadata": {},
   "source": [
    "Categorical variables can be collapsed into a fewer number of categories. One approach is to retain the values of the most frequently observed values and collapse all remaining values into a single category. For example, to retain only the values of the top 10 most frequent categories in the `department` column and then collapse the other categories together, use `value_counts` to generate the count of the values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b9602d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[\"department\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fec8e2",
   "metadata": {},
   "source": [
    "Next, select just the top 10 using the `head()` method, and select the column names by using the `index` attribute of the series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51582ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10 = df2[\"department\"].value_counts().head(10).index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cb5d08",
   "metadata": {},
   "source": [
    "Finally, use the apply method and a lambda function to select only the values from the `department` column and collapse the remaining values into the value `Other`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8ac779",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[\"department\"] = df2[\"department\"].apply(lambda x: x if x in top_10 else \"Other\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad385bb1",
   "metadata": {},
   "source": [
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Try it yourself! Retain the remaining top 5 most frequent values in the `department` column and collapse the remaining values into the category `Other`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43a7eb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c0fecb0",
   "metadata": {},
   "source": [
    "# Applying Functions to DataFrames and Series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef100131",
   "metadata": {},
   "source": [
    "`apply` is a useful method for to using one function on all the rows or columns of a DataFrame efficiently. Let's take the following real estate dataset as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9499d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data, only use the numerical columns\n",
    "df = pd.read_csv(\"data/colombia-real-estate-2.csv\", usecols=[\"area_m2\", \"price_cop\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb779db",
   "metadata": {},
   "source": [
    "By specifying the function inside `apply()`, we can transform the whole DataFrame. For example, I am calculating the square root of each row at each column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d723258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df.apply(np.sqrt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11f47be",
   "metadata": {},
   "source": [
    "Note you can also specify the `\"axis\"` inside `apply`. By default we have `axis=0`, which means we are applying the function to each column. We can also switch to `axis=1` if we want to apply the function to each row. See the following example showing the sum of all rows for each column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3624dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.apply(np.sum, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa99180",
   "metadata": {},
   "source": [
    "The following code will get the sum of all columns for each row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8953b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.apply(np.sum, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d3395e",
   "metadata": {},
   "source": [
    "By specifying the column name, we can also apply the function to a specific column or columns. Note that we can also specify index (row names) to only apply functions to specific rows, however, it is not common in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1056a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"area_m2\"].apply(np.sqrt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461a3902",
   "metadata": {},
   "source": [
    "We can assign the results to a new column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9004bd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"area_sqrt\"] = df[\"area_m2\"].apply(np.sqrt)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e61abc",
   "metadata": {},
   "source": [
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Try it yourself! Create a new column named 'sum_columns', which is the sum of all numerical columns in `df`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d1d1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sum_columns\"] = ...\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4750672a",
   "metadata": {},
   "source": [
    "`df.aggregate()`, or `df.agg()`, shares the same concept as `df.apply()` in terms of applying functions to a DataFrame, but `df.aggregate()` can only apply aggregate functions like `sum`, `mean`, `min`, `max`, etc. See the following example for more details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54613a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/colombia-real-estate-2.csv\", usecols=[\"area_m2\", \"price_cop\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599eba37",
   "metadata": {},
   "source": [
    "We can check what's the minimum number for each column calling the `min` aggregate function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e541aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.agg(\"min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b710ec78",
   "metadata": {},
   "source": [
    "Like `apply()`, we can also specify the `axis` argument to switch axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbf2ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.agg(\"min\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd6ac7f",
   "metadata": {},
   "source": [
    "We can apply aggregate function to a whole DataFrame using `df.agg()`, or specify the column name for a subset of DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60ee76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"area_m2\"].agg(\"min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb6ec6a",
   "metadata": {},
   "source": [
    "We can also apply a list of aggregate functions to a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da51116d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.agg([\"sum\", \"max\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c4f244",
   "metadata": {},
   "source": [
    "The syntax above will calculate both `sum` and `max` for each column, and store the result as index. Besides, we can also apply different aggregate functions to different columns. In this case, we need to pass a dictionary specifying key as column names, and value as corresponding aggregate function names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6c1918",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.agg({\"area_m2\": \"sum\", \"price_cop\": \"min\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d6dd7b",
   "metadata": {},
   "source": [
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Try it yourself! Find the minimum for column `area_m2` and the maximum for column `price_cop` using `df.agg()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b457b7ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "312beebd",
   "metadata": {},
   "source": [
    "# References & Further Reading\n",
    "- [Pandas Documentation on Dropping a Column in a DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html)\n",
    "- [Pandas Documentation on Printing out the First Few Rows of a DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.head.html)\n",
    "- [Pandas Documentation on Reading in a CSV File Into a DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html)\n",
    "- [Getting Started with Pandas Documentation](https://pandas.pydata.org/docs/getting_started/intro_tutorials/01_table_oriented.html#min-tut-01-tableoriented)\n",
    "- [Pandas Documentation on Extracting a Column to a Series](https://pandas.pydata.org/docs/getting_started/intro_tutorials/01_table_oriented.html#each-column-in-a-dataframe-is-a-series)\n",
    "- [Pandas Official Indexing Guide](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html)\n",
    "- [Series in pandas](https://www.geeksforgeeks.org/python-pandas-series/)\n",
    "- [Tutorial for `value_counts`](https://www.geeksforgeeks.org/python-pandas-series-value_counts/)\n",
    "- [Tutorial for `groupby`](https://pandas.pydata.org/pandas-docs/dev/reference/api/pandas.DataFrame.groupby.html)\n",
    "- [pandas Documentation for `groupby`](https://pandas.pydata.org/pandas-docs/version/0.25.3/reference/api/pandas.core.groupby.GroupBy.mean.html)\n",
    "- [Pandas Official Indexing Guide](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html)\n",
    "- [Online Examples of Selecting Numeric Columns of a DataFrame](https://pythonexamples.org/pandas-dataframe-select-columns-of-numeric-datatype/)\n",
    "- [Official Pandas Documentation on Data Types in a DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.select_dtypes.html)\n",
    "- [Pandas documentation for Boolean indexing](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#boolean-indexing)\n",
    "- [More information on creating various kinds of subsets](https://datacarpentry.org/python-ecology-lesson/03-index-slice-subset/index.html#subsetting-data-using-criteria)\n",
    "- [More on boolean indexing](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#boolean-indexing)\n",
    "- [A tutorial on using various criteria to create subsets](https://datacarpentry.org/python-ecology-lesson/03-index-slice-subset/index.html#subsetting-data-using-criteria)\n",
    "- [Pandas.DataFrame.apply](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html)\n",
    "- [Pandas.DataFrame.aggregate](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.aggregate.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
