{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a82467d",
   "metadata": {},
   "source": [
    "<font size=\"+3\"><strong>Time Series: Core Concepts</strong></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a8a7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df07795",
   "metadata": {},
   "source": [
    "# Model Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5e94a9",
   "metadata": {},
   "source": [
    "## Autoregression Models\n",
    "\n",
    "Autoregression (AR) is a time series model that uses observations from previous time steps as input to a regression equation to predict the value at the next time step. AR works in a similar way to **autocorrelation**: in both cases, we're taking data from one part of a set and comparing it to another part. An AR model regresses itself. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4dd571",
   "metadata": {},
   "source": [
    "## ARMA Models\n",
    "\n",
    "**ARMA** stands for Auto Regressive Moving Average, and it's a special kind of **time-series** analysis. So far, we've used autoregression (AR) to build our time-series models, and you might recall that AR models rely on values that remain relatively stable over time. That is, they can predict the future very well, as long as the future looks roughly the same as the past. The trouble with predicting the future is that things can suddenly change, and as a result, the future doesn't look much like the past anymore. These sudden changes &mdash; economists call them *endogenous shocks* &mdash; can be as big as a hurricane destroying a city or an unexpected increase in the minimum wage, and they can be as small as a new restaurant opening in a neighborhood or a single person losing their job. In our data, the air quality might be changed if there was a nearby forest fire, or if a building collapsed near one of the sensors and raised a giant cloud of dust. \n",
    "\n",
    "Below is a video from [Ritvik Kharkar](https://www.linkedin.com/in/ritvik-kharkar/) that explains MA models in terms of cupcakes and crazy professors â€” two things we love! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6e6a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTubeVideo(\"voryLhxiPzE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1ea476",
   "metadata": {},
   "source": [
    "And in this video, Ritvik talks about the ARMA model we use in Project 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f9cf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTubeVideo(\"HhvTlaN06AM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db2b8d9",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5271f993",
   "metadata": {},
   "source": [
    "## ACF Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a878c3",
   "metadata": {},
   "source": [
    "When we've worked with autocorrelations in the past, we've treated them like static relationships, but that's not always how they work. Sometimes, we want to actually see how those autocorrelations change over time, which means we need to think of them as *functions*. When we create a visual representation of an autocorrelation function (ACF), we're making an **ACF plot**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed8e195",
   "metadata": {},
   "source": [
    "## PACF Plot\n",
    "\n",
    "Autocorrelations take into account two types of observations. **Direct observations** are the ones that happen exactly at our chosen time-step interval; we might have readings at one-hour intervals starting at 1:00. **Indirect observations** are the ones that happen between our chosen time-step intervals, at time-steps like 1:38, 2:10, 3:04, etc. Those indirect observations *might* be helpful, but we can't be sure about that, so it's a good idea to strip them out and see what our graph looks like when it's only showing us direct observations. \n",
    "\n",
    "An autocorrelation that only includes the direct observations is called a **partial autocorrelation**, and when we view that partial autocorrelation as a function, we call it a **PACF**.\n",
    "\n",
    "**PACF plots** represent those things visually. We want to compare our ACF and PACF plots to see which model best describes our time series. If the ACF data drops off slowly, then that's going to be a better description; if the PACF falls off slowly, then that's going to be a better description."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5002cfd",
   "metadata": {},
   "source": [
    "# Statistical Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f230ba",
   "metadata": {},
   "source": [
    "## Walk-Forward Validation\n",
    "\n",
    "Our predictions lose power over time because the model gets farther and farther away from its beginning. But what if we could move that beginning forward with the model? That's what **walk-forward validation** is. In a walk-forward validation, we re-train the model at for each new observation in the dataset, dropping the data that's the farthest in the past. Let's say that our prediction for what's going to happen at 12:00 is based on what happened at 11:00, 10:00, and 9:00. When we move forward an hour to predict what's going to happen at 1:00, we only use data from 10:00, 11:00, and 12:00, dropping the data from 9:00 because it's now too far in the past."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd5457a",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "Parameters are the parts of the model that are **learned** from the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85424f2d",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "We've already seen that **parameters** are elements that a machine learning model learns from the training data. **Hyperparameters**, on the other hand, are elements of the model that come from somewhere else. Data scientists choose hyperparameters either by examining the data themselves, or by creating some kind of automated testing of different options to see how they perform. Hyperparameters are set before the model is trained, which means that they significantly impact how the model is trained, and how it subsequently performs. One way of thinking about the difference between the two is that parameters come from *inside* the model, and hyperparameters come from *outside* the model.\n",
    "\n",
    "When we think about hyperparameters, we think in terms of `p` values and `q` values. `p` values represent the number of lagged observations included in the model, and the `q` is the size of the moving average window. These values count as hyperparameters because we get to decide what they are. How many lagged observations do we want to include? How big should our window of moving averages be?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a2270c",
   "metadata": {},
   "source": [
    "## Rolling Averages\n",
    "\n",
    "A **rolling average** is the mean value of multiple subsets of numbers in a dataset. For example, I might have data relating to the daily income for a shop I own, and as long as the shop stays open, I can calculate a rolling average. On Friday, I might calculate the average income from Monday-Thursday. The next Monday, I might calculate the average income from Tuesday-Friday, and the next day, I might calculate the average income from Wednesday to Monday, and so on. These averages *roll*, giving me a sense for how the data is changing in relation to any kind of static construct. In this case, and in many data science applications, that construct is time. Calculating rolling averages is helpful for making accurate forecasts about the ways data will change in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d740356a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
