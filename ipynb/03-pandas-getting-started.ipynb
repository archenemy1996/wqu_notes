{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83bf244d",
   "metadata": {},
   "source": [
    "<font size=\"+3\"><strong>Pandas: Getting Started</strong></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd13ce6",
   "metadata": {},
   "source": [
    "# Pandas\n",
    "\n",
    "**Pandas** is a Python library used for working with datasets. It does that by helping us make sense of **DataFrames**, which are a form of two-dimensional **structured data**, like a table with columns and rows. But before we can do anything else, we need to start with data in a CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce850de",
   "metadata": {},
   "source": [
    "# Importing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0b9164",
   "metadata": {},
   "source": [
    "## CSV Files\n",
    "\n",
    "CSV stands for Comma Separated Values, and it's a file type that allows data to be saved in a table. Data presented in a table is called **structured data**, because it adheres to the idea that there is a meaningful relationship between the columns and rows. A CSV might also show **panel data**, which is data that shows observations of the same behavior at various different times. The datasets we're using in this part of the course are all structured tables, but you'll see other arrangements of data as you move through your projects.\n",
    "\n",
    "If you're familiar with the way data tables look in spreadsheet applications like Excel, you might be surprised to see that raw CSV files don't look like that. If you came across a CSV file and opened it to see what it looked like, you'd see something like this:\n",
    "\n",
    "```python\n",
    "property_type,department,lat,lon,area_m2,price_usd\n",
    "house,Bogotá D.C,4.69,-74.048,187.0,\"$330,899.98\"\n",
    "house,Bogotá D.C,4.695,-74.082,82.0,\"$121,555.09\"\n",
    "house,Quindío,4.535,-75.676,235.0,\"$219,474.47\"\n",
    "house,Bogotá D.C,4.62,-74.129,195.0,\"$97,919.38\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d7b5a3",
   "metadata": {},
   "source": [
    "## Dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54b5688",
   "metadata": {},
   "source": [
    "You can create a DataFrame from a Python dictionary using `from_dict` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5017b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\"col_1\": [3, 2, 1, 0], \"col_2\": [\"a\", \"b\", \"c\", \"d\"]}\n",
    "pd.DataFrame.from_dict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e42e7e",
   "metadata": {},
   "source": [
    "By default, DataFrame will be created using keys as columns. Note the length of the values should be equal for each key for the code to work. We can also let keys to be index instead of the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed02dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(data, orient=\"index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3e18a9",
   "metadata": {},
   "source": [
    "We can also specify column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec77915",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(data, orient=\"index\", columns=[\"A\", \"B\", \"C\", \"D\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db50f0e1",
   "metadata": {},
   "source": [
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Try it yourself! Create a DataFrame called using the dictionary `clothes` and make the keys as index, and put column names as ['color','size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4717cd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "clothes = {\"shirt\": [\"red\", \"M\"], \"sweater\": [\"yellow\", \"L\"], \"jacket\": [\"black\", \"L\"]}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a495df46",
   "metadata": {},
   "source": [
    "## JSON Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7e8f9b",
   "metadata": {},
   "source": [
    "JSON is short for JavaScript Object Notation. It is another widely used data format to store and transfer the data. It is light-weight and very human readable. In Python, we can use the `json` library to read JSON files. Here is an example of a JSON string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0a29d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "info = \"\"\"{\n",
    "    \"firstName\": \"Jane\",\n",
    "    \"lastName\": \"Doe\",\n",
    "    \"hobby\": \"running\",\n",
    "    \"age\": 35\n",
    "}\"\"\"\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44069872",
   "metadata": {},
   "source": [
    "Use `json` library to load the json string into a Python dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb20a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data = json.loads(info)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ae40fc",
   "metadata": {},
   "source": [
    "We can load a json string or file into a dictionary because they are organized in the same way: key-value pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e150b50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"firstName\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd9d05e",
   "metadata": {},
   "source": [
    "A dictionary may not be as convenient as a `DataFrame` in terms of data manipulation and cleaning. But once we've turned our json string into a dictionary, we can transform it into a `DataFrame` using the `from_dict` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3ce585",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(data, orient=\"index\", columns=[\"subject 1\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8913df15",
   "metadata": {},
   "source": [
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Try it yourself! Load the JSON file `clothes` and then transform it to `DataFrame`, name column properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f44f6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "clothes = \"\"\"{\"shirt\": [\"red\",\"M\"], \"sweater\": [\"yellow\",\"L\"]}\"\"\"\n",
    "\n",
    "\n",
    "data = ...\n",
    "df = ...\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8026041e",
   "metadata": {},
   "source": [
    "# Load Compressed file in Python\n",
    "\n",
    "In the big data era, it is very likely that we'll need to read data from compressed files. One way to unzip the data is to use gzip. We can load the `poland-bankruptcy-data-2008.json.gz` file from the data folder using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6588f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "\n",
    "with gzip.open(\"data/poland-bankruptcy-data-2008.json.gz\", \"r\") as f:\n",
    "    poland_data_gz = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c6507d",
   "metadata": {},
   "source": [
    "`poland_data_gz` is a dictionary, and we only need the `data` portion of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04bb8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "poland_data_gz.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff54e60",
   "metadata": {},
   "source": [
    "We can use the `from_dict` function from pandas to read the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa92d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame().from_dict(poland_data_gz[\"data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ef267e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e441b109",
   "metadata": {},
   "source": [
    "<font size=\"+1\">Practice</font>\n",
    " \n",
    "Read `poland-bankruptcy-data-2007.json.gz` into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0196ad9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load file into dictionary\n",
    "\n",
    "\n",
    "# Transform dictionary into DataFrame\n",
    "df = ...\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cdfd8a",
   "metadata": {},
   "source": [
    "## Pickle Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5e92e1",
   "metadata": {},
   "source": [
    "Pickle in Python is primarily used in `serializing` and `deserializing` a Python object structure. `Serialization` is the process of turning an object in memory into a stream of bytes so you can store it on disk or send it over a network. `Deserialization` is the reverse process: turning a stream of bytes back into an object in memory.\n",
    "\n",
    "According to the pickle module documentation, the following types can be pickled:\n",
    "\n",
    "* `None`\n",
    "* Booleans\n",
    "* Integers, long integers, floating point numbers, complex numbers\n",
    "* Normal and Unicode strings\n",
    "* Tuples, lists, sets, and dictionaries containing only objects that can be pickled\n",
    "* Functions defined at the top level of a module\n",
    "* Built-in functions defined at the top level of a module\n",
    "* Classes that are defined at the top level of a module\n",
    "\n",
    "Let's demonstrate using a python dictionary as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f406e343",
   "metadata": {},
   "outputs": [],
   "source": [
    "clothes = {\"shirt\": [\"red\", \"M\"], \"sweater\": [\"yellow\", \"L\"], \"jacket\": [\"black\", \"L\"]}\n",
    "clothes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923714e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(clothes, open(\"./data/clothes.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1470354a",
   "metadata": {},
   "source": [
    "Now in the data folder, there will be a file named `clothes.pkl`. We can read the pickled file using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f7ee92",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/clothes.pkl\", \"rb\") as f:\n",
    "    unpickled = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074da228",
   "metadata": {},
   "outputs": [],
   "source": [
    "unpickled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfff608",
   "metadata": {},
   "source": [
    "Note first we are using `wb` inside the `open` function because we are creating this file, while `deserializing` the file, we are using `rb` to read the file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523bd84c",
   "metadata": {},
   "source": [
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Store the sample list into a pickle file, and load the pickle file back to a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc25e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_list = [1, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9fcb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "unpickled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fda697",
   "metadata": {},
   "source": [
    "# Working with DataFrames\n",
    "\n",
    "The first thing we need to do is import pandas; we'll use `pd` as an *alias* when we include it in our code.\n",
    "\n",
    "Pandas is just a library; to get anything done, we need a dataset too. We'll use the `read_csv` method to create a DataFrame from a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5dce4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/colombia-real-estate-1.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb869f74",
   "metadata": {},
   "source": [
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Try it yourself! Create a DataFrame called `df2` using the `colombia-real-estate-2` CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e7cd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = ...\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864f2a42",
   "metadata": {},
   "source": [
    "# Working with DataFrame Indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23801ff7",
   "metadata": {},
   "source": [
    "A DataFrame stores data in a row-and-column format. The DataFrame Index is a special kind of column that helps identify the location of each row. The default Index uses integers starting at zero, but you can also set up customized indices like `\"name\"`, `\"location\"`, etc. For example, in the following real estate data set, the default index are the integer counts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4482e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = ...\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809c47b9",
   "metadata": {},
   "source": [
    "We can call the index column through `.index`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daed7bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a63c1f",
   "metadata": {},
   "source": [
    "Use the `set_index` method, we can set the column `department` as the index instead. Note index column cannot have duplicate rows, like here we cannot set `property_type` as the index column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd79526",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index(\"department\", inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cb2620",
   "metadata": {},
   "source": [
    "Now you can see the index column has changed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6679c0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a208719",
   "metadata": {},
   "source": [
    "Using the `reset_index()` function, we can reset index back to default integer counts, and `department` will become a column again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84661da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43551055",
   "metadata": {},
   "source": [
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Try it yourself! Set `letter` as the index, then call the index. Then reset the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c9edb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"letter\": [\"a\", \"b\", \"c\", \"d\"],\n",
    "    \"number\": [3, 2, 1, 0],\n",
    "    \"location\": [\"east\", \"east\", \"east\", \"west\"],\n",
    "}\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "\n",
    "# set index 'numbers'\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308c68c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e847c3",
   "metadata": {},
   "source": [
    "# Inspecting DataFrames\n",
    "Once we've created a DataFrame, we need to **inspect** it in order to see what's there. Pandas has many ways to inspect a DataFrame, but we're only going to look at three of them: `shape`, `info`, and `head`.\n",
    "\n",
    "If we're interested in understanding the **dimensionality** of the DataFrame, we can use the `df.shape` method. The code looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe277cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d5571c",
   "metadata": {},
   "source": [
    "The `shape` output tells us that the `colombia-real-estate-1` DataFrame -- which we called `df1` -- has 3066 rows and 6 columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c6c0e3",
   "metadata": {},
   "source": [
    "If we're trying to get a **general idea** of what the DataFrame contained, we can use the `info` method. The code looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f18543",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9933d30",
   "metadata": {},
   "source": [
    "The `info` output tells us all sorts of things about the DataFrame: the number of columns, the names of the columns, the data type for each column, how many non-null rows are contained in the DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3de8c49",
   "metadata": {},
   "source": [
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Try it yourself! Use `info` and `shape` to explore `df2`, which you created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8690ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1adb631c",
   "metadata": {},
   "source": [
    "If we wanted to see all the rows in our new DataFrame, we could use the `print` method. Keep in mind that the entire dataset gets printed when you use `print`, even though it only shows you the first few lines. That's not much of a problem with this particular dataset, but once you start working with much bigger datasets, printing the whole thing will cause all sorts of problems. \n",
    "\n",
    "So instead of doing that, we'll just take a look at the first five rows by using the `head` method. The code looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0264df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbfe4f1",
   "metadata": {},
   "source": [
    "By default, `head` returns the first five rows of data, but you can specify as many rows as you like. Here's what the code looks like for just the first two rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b264389b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9f2552",
   "metadata": {},
   "source": [
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Try it yourself! Use the `head` method to return the first five and first 7 rows of the `colombia-real-estate-2` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca19475",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d92ff26",
   "metadata": {},
   "source": [
    "# Working with Columns\n",
    "\n",
    "Sometimes, it’s handy to duplicate a column of data. It might be that you’d like to drop some data points or erase empty cells while still preserving the original column. If you’d like to do that, you’ll need to duplicate the column. We can do this by placing the name of the new column in square brackets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca30b1eb",
   "metadata": {},
   "source": [
    "## Adding Columns\n",
    "\n",
    "For example, we might want to add a column of data that shows the price per square meter of each house in US dollars. To do that, we're going to need to create a new column, and include the necessary math to populate it. First, we need to import the CSV and inspect the first five rows using the `head` method, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8db56c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv(\"data/colombia-real-estate-3.csv\")\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4407d6bd",
   "metadata": {},
   "source": [
    "Then, we create a new column called `\"price_m2\"`, provide the formula to populate it, and inspect the first five rows of the dataset to make sure the new column includes the new values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8f0d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3[\"price_m2\"] = df3[\"price_usd\"] / df3[\"area_m2\"]\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fc44ca",
   "metadata": {},
   "source": [
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Try it yourself! Add a column to the `colombia-real-estate-2` dataset that shows the price per square meter of each house in Colombian pesos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e18af4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ...\n",
    "df[\"price_m2\"] = ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d2e3f4",
   "metadata": {},
   "source": [
    "## Dropping Columns\n",
    "\n",
    "Just like we can add columns, we can also take them away. To do this, we’ll use the `drop` method. If I wanted to drop the `“department”` column from `colombia-real-estate-1`, the code would look like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dac8a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.drop(\"department\", axis=\"columns\")\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4964bd4",
   "metadata": {},
   "source": [
    "Note that we specified that we wanted to drop a column by setting the `axis` argument to `\"columns\"`. We can drop rows from the dataset if we change the `axis` argument to `\"index\"`. If we wanted to drop row 2 from the `df2` data, the code would look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a75e04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.drop(2, axis=\"index\")\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed88993b",
   "metadata": {},
   "source": [
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Try it yourself! Drop the `\"property_type\"` column and row 4 in the `colombia-real-estate-2` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31335a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1626d6e0",
   "metadata": {},
   "source": [
    "## Dropping Rows\n",
    "\n",
    "Including rows with empty cells can radically skew the results of our analysis, so we often drop them from the dataset. We can do this with the `dropna` method. If we wanted to do this with `df`, the code would look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643b45d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"df shape before dropping rows\", df.shape)\n",
    "df.dropna(inplace=True)\n",
    "print(\"df shape after dropping rows\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f4ecc2",
   "metadata": {},
   "source": [
    "By default, pandas will keep the original DataFrame, and will create a copy that reflects the changes we just made. That's perfectly fine, but if we want to make sure that copies of the DataFrame aren't clogging up the memory on our computers, then we need to intervene with the `inplace` argument. `inplace=True` means that we want the original DataFrame updated without making a copy. If we don't include `inplace=True` (or if we do include `inplace=False`), then pandas will revert to the default. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15be1ce",
   "metadata": {},
   "source": [
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Drop rows with empty cells from the `colombia-real-estate-2` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b2faac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f641c8",
   "metadata": {},
   "source": [
    "## Splitting Strings\n",
    "\n",
    "It might be useful to split strings into their constituent parts, and create new columns to contain them. To do this, we’ll use the `.str.split` method, and include the character we want to use as the place where the data splits apart. In the `colombia-real-estate-3` dataset, we might be interested breaking the `\"lat-lon\"` column into a `\"lat\"` column and a `\"lon\"` column. We’ll split it at `“,”` with code that looks like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b303180",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3[[\"lat\", \"lon\"]] = df3[\"lat-lon\"].str.split(\",\", expand=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba4dcaa",
   "metadata": {},
   "source": [
    "Here, `expand` is telling pandas to make the DataFrame bigger; that is, to create a new column without dropping any of the ones that already exist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1333bc",
   "metadata": {},
   "source": [
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Try it yourself! In `df3`, split `\"place_with_parent_names\"` into three columns (one called `\"place\"`, one called `\"department\"`, and one called `\"state\"`, using the character `“|”`, and then return the new `\"department\"` column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95ecc0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71656a3c",
   "metadata": {},
   "source": [
    "## Recasting Data\n",
    "\n",
    "Depending on who formatted your dataset, the types of data assigned to each column might need to be changed. If, for example, a column containing only numbers had been mistaken for a column containing only strings, we’d need to change that through a process called *recasting*. Using the `colombia-real-estate-1` dataset, we could recast the entire dataset as strings by using the `astype` method, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2fafe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.info())\n",
    "newdf = df.astype(\"str\")\n",
    "print(newdf.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7540cf6",
   "metadata": {},
   "source": [
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Try it yourself! In the `colombia-real-estate-2` dataset, recast `\"price_cop\"` as an object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cf0cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ...\n",
    "df2[\"price_cop\"] = ...\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8023039c",
   "metadata": {},
   "source": [
    "## Access a substring in a Series\n",
    "\n",
    "To access a substring from a Series, use the `.str` attribute from the Series. Then, index each string in the Series by providing the `start:stop:step`. Keep in mind that the start position is inclusive and the stop position is exclusive, meaning the value at the start index is included but the value at the stop index is not included. Also, Python is a 0-indexed language, so the first element in the substring is at index position 0. For example, using the `colombia-real-estate-1` dataset, we could the values at index position 0, 2, and 4 of the `department` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780294b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"department\"].str[0:5:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924d15c6",
   "metadata": {},
   "source": [
    "<font size=\"+1\">Practice: Access a substring in a Series using pandas</font>\n",
    "\n",
    "Try it yourself! In the `colombia-real-estate-2` dataset, access the `property_type` column and return the first 5 characters from each row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f760ded9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45132b84",
   "metadata": {},
   "source": [
    "## Replacing String Characters\n",
    "\n",
    "Another change you might want to make is replacing the characters in a string. To do this, we’ll use the `replace` method again, being sure to specify which string should be replaced, and what new string should replace it. For example, if we wanted to replace the string `“house”` with the string `“single_family”` in the `colombia-real-estate-1` dataset, the code would look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08db7ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"property_type\"] = df[\"property_type\"].str.replace(\"house\", \"single_family\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab198e6",
   "metadata": {},
   "source": [
    "There are two important things to note here. The first is that the old value needs to come before the new value inside the parentheses of `str.replace`. \n",
    "\n",
    "The second important issue here is that, unless you specify differently, *all* instances of the old value will be replaced. If you only want to replace the first three instances, the code would look like this: `str.replace(“house”, “single_family”, 3)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8899be66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"property_type\"] = df[\"property_type\"].str.replace(\"house\", \"single_family\", 3)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c6e6aa",
   "metadata": {},
   "source": [
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Try it yourself! In the `colombia-real-estate-2` dataset, change `“apartment”` to `“multi_family”`, in the first 7 rows, and print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30df4c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ba2490",
   "metadata": {},
   "source": [
    "### Rename a Series\n",
    "\n",
    "Another change you might want to make is to rename a Series in pandas. To do this, we’ll use the `rename` method, being sure to specify the mapping of old and new columns. For example, if we wanted to replace the column name `property_type` with the string `type_property` in the `colombia-real-estate-1` dataset, the code would look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5177a57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={\"property_type\": \"type_property\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b70e50",
   "metadata": {},
   "source": [
    "<font size=\"+1\">Practice: Rename a Series</font>\n",
    "\n",
    "Try it yourself! In the `colombia-real-estate-2` dataset, change the column `lat` to `latitude` and print the head of DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223dd6a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79337a04",
   "metadata": {},
   "source": [
    "### Determine the unique values in a column\n",
    "\n",
    "You might be interested in the unique values in a Series using pandas. To do this, we’ll use the `unique` method. For example, if we wanted to identify the unique values in the column  `property_type` in the `colombia-real-estate-1` dataset, the code would look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aae4159",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"property_type\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4f47e0",
   "metadata": {},
   "source": [
    "<font size=\"+1\">Practice: Determine the unique values in a column</font>\n",
    "\n",
    "Try it yourself! In the `colombia-real-estate-2` dataset, identify the unique values in the column  `department`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6955cd58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf92add1",
   "metadata": {},
   "source": [
    "## Replacing Column Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723519b9",
   "metadata": {},
   "source": [
    "If you want to replace a columns' values, simply use the `.replace()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2f5a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Series.rename() example\n",
    "df = pd.read_csv(\"data/colombia-real-estate-2.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94424f98",
   "metadata": {},
   "source": [
    "We can replace a specific row with other values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e872b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"area_m2\"].replace(235.0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c50d2d",
   "metadata": {},
   "source": [
    "If you want to replace multiple values at the same time, you can also define a dictionary ahead of time, with dictionary keys the originals and dictionary values the replaced values. Then pass the dictionary to the `replace()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712151bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_value = {235: 0, 130: 1, 137: 2}\n",
    "\n",
    "df[\"area_m2\"].replace(replace_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cafa80b",
   "metadata": {},
   "source": [
    "Or we can apply specific operations to a whole column. In the following example, we have changed the `price_cop` unit to millions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463a41ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"price_cop\"] = df[\"price_cop\"] / 1e6\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6417ae2d",
   "metadata": {},
   "source": [
    "<font size=\"+1\">Practice: Replace Column Values</font>\n",
    "\n",
    "Try it yourself! Define a dictionary to replace values in `price_cop`. Replace 400 to 0, 850 to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b76e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_value = ...\n",
    "\n",
    "# Replace values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd43405b",
   "metadata": {},
   "source": [
    "# Concatenating\n",
    "\n",
    "When we **concatenate** data, we're combining two or more separate sets of data into a single large dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f15dce",
   "metadata": {},
   "source": [
    "## Concatenating DataFrames\n",
    "\n",
    "If we want to combine two DataFrames, we need to import Pandas and read in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18300f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"data/colombia-real-estate-1.csv\")\n",
    "df2 = pd.read_csv(\"data/colombia-real-estate-2.csv\")\n",
    "print(\"df1 shape:\", df1.shape)\n",
    "print(\"df2 shape:\", df2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a483e7",
   "metadata": {},
   "source": [
    "Next, we'll use the `concat` method to put our DataFrames together, using each DataFrame's name in a list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9c6f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_df = pd.concat([df1, df2])\n",
    "print(\"concat_df shape:\", concat_df.shape)\n",
    "concat_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94d25c1",
   "metadata": {},
   "source": [
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Try it yourself! Create two DataFrames from `colombia-real-estate-2.csv` and `colombia-real-estate-3.csv`, and concatenate them as the DataFrame `concat_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4b7955",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = ...\n",
    "df3 = ...\n",
    "concat_df = ...\n",
    "concat_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3b0afe",
   "metadata": {},
   "source": [
    "## Concatenating Series\n",
    "\n",
    "We can also concatenate a Series using a similar set of commands. First, let's take two Series from the `df1` and `df2` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497bfec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"data/colombia-real-estate-1.csv\")\n",
    "df2 = pd.read_csv(\"data/colombia-real-estate-2.csv\")\n",
    "sr1 = df1[\"property_type\"]\n",
    "sr2 = df2[\"property_type\"]\n",
    "print(\"len sr1:\", len(sr1)),\n",
    "print(sr1.head())\n",
    "print()\n",
    "print(\"len sr2:\", len(sr2)),\n",
    "print(sr2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18baa572",
   "metadata": {},
   "source": [
    "Now that we have two Series, let's put them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d803f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_sr = pd.concat([sr1, sr2])\n",
    "print(\"len concat_sr:\", len(concat_sr)),\n",
    "print(concat_sr.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a5c632",
   "metadata": {},
   "source": [
    "<font size=\"+1\">Practice</font>\n",
    "\n",
    "Try it yourself! Use the `colombia-real-estate-2` and `colombia-rea-estate-3` datasets to create a concatenated Series for the `area_m2` column, and print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418653a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5b4b72",
   "metadata": {},
   "source": [
    "# Saving a DataFrame as a CSV\n",
    "Once you’ve cleaned all your data and gotten the DataFrame to show everything you want it to show, it’s time to save the DataFrame as a new CSV file using the [`to_csv`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html?highlight=to_csv#pandas.DataFrame.to_csv) method. First, let's load up the `colombia-real-estate-1` dataset, and use `head` to see the first five rows of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bca6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/colombia-real-estate-1.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90c62fd",
   "metadata": {},
   "source": [
    "Maybe we're only interested in those first five rows, so let's save that as its own new CSV file using the `to_csv` method. Note that we're setting the `index` argument to `False` so that the DataFrame index isn't included in the CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb060cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.head()\n",
    "df.to_csv(\"data/small-df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e46818b",
   "metadata": {},
   "source": [
    "# References & Further Reading \n",
    "\n",
    "- [Tutorial for `shape`](https://www.w3resource.com/pandas/dataframe/dataframe-shape.php)\n",
    "- [Tutorial for `info`](https://www.w3schools.com/python/pandas/ref_df_info.asp)\n",
    "- [Adding columns to a DataFrame](https://pandas.pydata.org/pandas-docs/version/1.0.5/getting_started/intro_tutorials/05_add_columns.html)\n",
    "- [Creating DataFrame from dictionary](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.from_dict.html)\n",
    "- [Working with JSON](https://realpython.com/python-json/)\n",
    "- [Dropping columns from a DataFrame](https://www.w3schools.com/python/pandas/ref_df_drop.asp)\n",
    "- [Splitting columns in a DataFrame](https://www.geeksforgeeks.org/split-a-text-column-into-two-columns-in-pandas-dataframe/)\n",
    "- [Recasting values](https://www.w3schools.com/Python/pandas/ref_df_astype.asp)\n",
    "- [Replacing strings](https://www.w3schools.com/python/ref_string_replace.asp)\n",
    "- [Concatenating DataFrames](https://cmdlinetips.com/2020/04/how-to-concatenate-two-or-more-pandas-dataframes/)\n",
    "- [From DataFrames to Series](https://datatofish.com/pandas-dataframe-to-series/)\n",
    "- [Stack Overflow: What is serialization](https://stackoverflow.com/questions/633402/what-is-serialization)\n",
    "- [Understand Python Pickling](https://www.synopsys.com/blogs/software-security/python-pickling/#:~:text=Pickle%20in%20Python%20is%20primarily,transport%20data%20over%20the%20network.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19f6840",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
